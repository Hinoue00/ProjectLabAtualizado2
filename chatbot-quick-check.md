# ğŸ¤– LabConnect - VerificaÃ§Ã£o RÃ¡pida do Chatbot

## ğŸ” **Comandos para Verificar no Servidor**

### **1. Verificar se Ollama estÃ¡ instalado**
```bash
ollama --version
# Esperado: ollama version is 0.x.x
```

### **2. Verificar se serviÃ§o estÃ¡ rodando**
```bash
pgrep -f "ollama serve"
# Esperado: nÃºmero do processo

# Ou verificar porta
netstat -tlnp | grep 11434
# Esperado: linha mostrando porta 11434 LISTEN
```

### **3. Testar conectividade Ollama**
```bash
curl http://localhost:11434/api/version
# Esperado: {"version":"0.x.x"}

curl http://localhost:11434/api/tags
# Esperado: lista de modelos instalados
```

### **4. Verificar modelos instalados**
```bash
ollama list
# Esperado: lista com llama3 ou outros modelos
```

### **5. Testar modelo especÃ­fico**
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "prompt": "Diga apenas: LabConnect funcionando",
    "stream": false
  }'
```

### **6. Verificar variÃ¡veis de ambiente Django**
```bash
cd /var/www/labconnect
grep -E "OLLAMA|ollama" .env
# Esperado: 
# OLLAMA_API_URL=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama3
```

### **7. Testar endpoint Django**
```bash
# Testar se endpoint responde
curl -X GET https://seu-dominio/api/assistant/
# Ou localhost se testando local:
curl -X GET http://localhost:8000/api/assistant/
```

---

## ğŸ› ï¸ **Comandos de CorreÃ§Ã£o**

### **Se Ollama nÃ£o estiver instalado:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### **Se serviÃ§o nÃ£o estiver rodando:**
```bash
# Tentar como serviÃ§o
sudo systemctl start ollama
sudo systemctl enable ollama

# Ou iniciar manualmente
nohup ollama serve > /tmp/ollama.log 2>&1 &
```

### **Se modelo nÃ£o estiver instalado:**
```bash
ollama pull llama3
# Ou modelo menor para teste:
ollama pull phi
```

### **Se variÃ¡veis nÃ£o estiverem configuradas:**
```bash
echo "OLLAMA_API_URL=http://localhost:11434/api/chat" >> .env
echo "OLLAMA_MODEL=llama3" >> .env
```

---

## ğŸ”§ **Script AutomÃ¡tico**

### **Executar verificaÃ§Ã£o completa:**
```bash
cd /var/www/labconnect
./setup-chatbot-ia.sh
```

### **VerificaÃ§Ã£o manual rÃ¡pida:**
```bash
# 1. Ollama OK?
ollama --version && echo "âœ… Ollama instalado"

# 2. ServiÃ§o OK?
pgrep -f "ollama serve" && echo "âœ… ServiÃ§o rodando"

# 3. API OK?
curl -s http://localhost:11434/api/version && echo "âœ… API respondendo"

# 4. Modelo OK?
ollama list | grep -q llama3 && echo "âœ… Modelo llama3 instalado"

# 5. Django OK?
python manage.py shell -c "
from django.conf import settings
print('âœ… OLLAMA_API_URL:', getattr(settings, 'OLLAMA_API_URL', 'NÃƒO CONFIGURADO'))
print('âœ… OLLAMA_MODEL:', getattr(settings, 'OLLAMA_MODEL', 'NÃƒO CONFIGURADO'))
"
```

---

## ğŸ“Š **Status Esperado (Tudo OK)**

```bash
âœ… Ollama: ollama version is 0.1.x
âœ… ServiÃ§o: processo rodando na porta 11434
âœ… API: {"version":"0.1.x"}
âœ… Modelo: llama3 disponÃ­vel
âœ… Django: variÃ¡veis configuradas
âœ… Endpoint: /api/assistant/ respondendo
```

---

## ğŸš¨ **Troubleshooting Comum**

| Problema | Comando de VerificaÃ§Ã£o | SoluÃ§Ã£o |
|----------|----------------------|---------|
| Ollama nÃ£o instalado | `ollama --version` | `curl -fsSL https://ollama.ai/install.sh \| sh` |
| ServiÃ§o parado | `pgrep -f "ollama serve"` | `ollama serve &` |
| Porta ocupada | `netstat -tlnp \| grep 11434` | Matar processo ou mudar porta |
| Modelo nÃ£o encontrado | `ollama list` | `ollama pull llama3` |
| Django nÃ£o conecta | Logs do Django | Verificar URL e firewall |

---

## ğŸ“ **Logs Importantes**

```bash
# Logs do Ollama
tail -f /tmp/ollama.log

# Logs do Django
tail -f /var/www/labconnect/logs/labconnect.log

# Logs do sistema
journalctl -u ollama -f
```

---

## ğŸ¯ **Teste Final**

ApÃ³s configurar tudo, teste com:

```bash
curl -X POST http://localhost:8000/api/assistant/ \
  -H "Content-Type: application/json" \
  -H "X-CSRFToken: seu-token" \
  -d '{"message": "OlÃ¡, vocÃª estÃ¡ funcionando?"}'
```

Se retornar resposta em streaming, **estÃ¡ funcionando!** ğŸ‰