#!/bin/bash
# Script de Configura√ß√£o Completa do Chatbot IA - LabConnect
# Verifica e configura Ollama + modelos necess√°rios

set -e

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Fun√ß√£o para log
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warning() {
    echo -e "${YELLOW}[WARNING] $1${NC}"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}"
}

info() {
    echo -e "${BLUE}[INFO] $1${NC}"
}

step() {
    echo -e "${PURPLE}[STEP] $1${NC}"
}

echo -e "${BLUE}ü§ñ LabConnect - Configura√ß√£o Completa do Chatbot IA${NC}"
echo "============================================="

# Verificar se est√° no diret√≥rio correto
if [[ ! -f "manage.py" ]]; then
    error "Script deve ser executado no diret√≥rio raiz do projeto Django"
    exit 1
fi

step "1. Verificando configura√ß√µes atuais..."

# Verificar vari√°veis de ambiente
log "Verificando vari√°veis de ambiente..."
echo "OLLAMA_API_URL: ${OLLAMA_API_URL:-'http://localhost:11434/api/chat (default)'}"
echo "OLLAMA_MODEL: ${OLLAMA_MODEL:-'llama3 (default)'}"

step "2. Verificando se Ollama est√° instalado..."

if command -v ollama &> /dev/null; then
    log "‚úÖ Ollama encontrado: $(ollama --version)"
else
    warning "‚ùå Ollama n√£o est√° instalado!"
    echo ""
    echo "Para instalar o Ollama:"
    echo "curl -fsSL https://ollama.ai/install.sh | sh"
    echo ""
    echo "Ou em Ubuntu/Debian:"
    echo "wget -O - https://ollama.ai/install.sh | bash"
    echo ""
    read -p "Deseja que eu tente instalar automaticamente? (y/n): " install_ollama
    
    if [[ $install_ollama =~ ^[Yy]$ ]]; then
        log "Instalando Ollama..."
        curl -fsSL https://ollama.ai/install.sh | sh
        
        if command -v ollama &> /dev/null; then
            log "‚úÖ Ollama instalado com sucesso!"
        else
            error "‚ùå Falha na instala√ß√£o do Ollama"
            exit 1
        fi
    else
        error "Ollama √© necess√°rio para o chatbot. Instale manualmente e execute novamente."
        exit 1
    fi
fi

step "3. Verificando servi√ßo Ollama..."

# Verificar se o servi√ßo est√° rodando
if pgrep -f "ollama serve" > /dev/null; then
    log "‚úÖ Servi√ßo Ollama est√° rodando"
else
    warning "‚ùå Servi√ßo Ollama n√£o est√° rodando"
    log "Iniciando servi√ßo Ollama..."
    
    # Tentar iniciar como servi√ßo systemd
    if systemctl list-unit-files | grep -q ollama; then
        sudo systemctl start ollama
        sudo systemctl enable ollama
        log "‚úÖ Servi√ßo Ollama iniciado via systemd"
    else
        # Iniciar manualmente em background
        nohup ollama serve > /tmp/ollama.log 2>&1 &
        sleep 3
        log "‚úÖ Servi√ßo Ollama iniciado manualmente"
    fi
fi

step "4. Testando conectividade com Ollama..."

# Testar conex√£o
OLLAMA_URL=${OLLAMA_API_URL:-"http://localhost:11434"}
BASE_URL=$(echo $OLLAMA_URL | sed 's|/api/chat||')

if curl -s "$BASE_URL/api/version" > /dev/null; then
    VERSION=$(curl -s "$BASE_URL/api/version" | grep -o '"version":"[^"]*' | cut -d'"' -f4)
    log "‚úÖ Ollama respondendo - Vers√£o: $VERSION"
else
    error "‚ùå Ollama n√£o est√° respondendo em $BASE_URL"
    error "Verifique se o servi√ßo est√° rodando: ollama serve"
    exit 1
fi

step "5. Verificando modelos dispon√≠veis..."

# Listar modelos instalados
MODELS=$(curl -s "$BASE_URL/api/tags" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = [model['name'] for model in data.get('models', [])]
    print(' '.join(models))
except:
    print('')
")

if [[ -n "$MODELS" ]]; then
    log "‚úÖ Modelos instalados: $MODELS"
else
    warning "‚ùå Nenhum modelo encontrado"
fi

step "6. Configurando modelo recomendado..."

# Modelo recomendado para o LabConnect
RECOMMENDED_MODEL="llama3"
MODEL_SIZE="4.7GB"

if echo "$MODELS" | grep -q "$RECOMMENDED_MODEL"; then
    log "‚úÖ Modelo $RECOMMENDED_MODEL j√° est√° instalado"
else
    warning "‚ùå Modelo $RECOMMENDED_MODEL n√£o encontrado"
    echo ""
    info "O modelo $RECOMMENDED_MODEL ($MODEL_SIZE) √© recomendado para o LabConnect"
    info "Oferece bom desempenho e respostas em portugu√™s"
    echo ""
    read -p "Deseja baixar o modelo $RECOMMENDED_MODEL? (y/n): " download_model
    
    if [[ $download_model =~ ^[Yy]$ ]]; then
        log "Baixando modelo $RECOMMENDED_MODEL..."
        log "‚è≥ Isso pode demorar alguns minutos dependendo da sua conex√£o..."
        
        if ollama pull $RECOMMENDED_MODEL; then
            log "‚úÖ Modelo $RECOMMENDED_MODEL baixado com sucesso!"
        else
            error "‚ùå Falha ao baixar o modelo"
        fi
    fi
fi

step "7. Testando comunica√ß√£o com modelo..."

# Testar modelo
TEST_MODEL=${OLLAMA_MODEL:-$RECOMMENDED_MODEL}

log "Testando modelo: $TEST_MODEL"

TEST_RESPONSE=$(curl -s "$BASE_URL/api/generate" \
    -H "Content-Type: application/json" \
    -d "{
        \"model\": \"$TEST_MODEL\",
        \"prompt\": \"Responda apenas: LabConnect funcionando\",
        \"stream\": false
    }" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print(data.get('response', '').strip())
except:
    print('ERROR')
")

if [[ "$TEST_RESPONSE" == *"LabConnect"* ]]; then
    log "‚úÖ Modelo respondendo corretamente: $TEST_RESPONSE"
else
    warning "‚ö†Ô∏è Resposta inesperada: $TEST_RESPONSE"
fi

step "8. Configurando vari√°veis de ambiente..."

# Criar arquivo .env se n√£o existir
if [[ ! -f ".env" ]]; then
    log "Criando arquivo .env..."
    touch .env
fi

# Verificar/adicionar configura√ß√µes
ENV_UPDATED=false

if ! grep -q "OLLAMA_API_URL" .env; then
    echo "OLLAMA_API_URL=http://localhost:11434/api/chat" >> .env
    ENV_UPDATED=true
fi

if ! grep -q "OLLAMA_MODEL" .env; then
    echo "OLLAMA_MODEL=$RECOMMENDED_MODEL" >> .env
    ENV_UPDATED=true
fi

if [[ $ENV_UPDATED == true ]]; then
    log "‚úÖ Vari√°veis de ambiente atualizadas no .env"
else
    log "‚úÖ Vari√°veis de ambiente j√° configuradas"
fi

step "9. Testando integra√ß√£o Django..."

log "Testando endpoint do Django..."

# Testar se o Django consegue se comunicar com o Ollama
python manage.py shell << EOF
import os
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'LabConnect.settings.production')
import django
django.setup()

from django.conf import settings
import requests

try:
    api_url = getattr(settings, 'OLLAMA_API_URL', 'http://localhost:11434/api/chat')
    base_url = api_url.rsplit('/', 2)[0]
    response = requests.get(f"{base_url}/api/version", timeout=5)
    if response.status_code == 200:
        print("‚úÖ Django consegue conectar com Ollama")
    else:
        print("‚ùå Django n√£o consegue conectar com Ollama")
except Exception as e:
    print(f"‚ùå Erro na conex√£o Django-Ollama: {e}")
EOF

step "10. Verificando URLs do chatbot..."

# Verificar se as URLs est√£o configuradas
if grep -q "api/" LabConnect/urls.py; then
    log "‚úÖ URLs da API configuradas"
else
    warning "‚ùå URLs da API podem n√£o estar configuradas"
fi

step "11. Gerando relat√≥rio de configura√ß√£o..."

cat > chatbot-config-report.txt << EOF
LabConnect - Relat√≥rio de Configura√ß√£o do Chatbot IA
===================================================
Data: $(date)

Configura√ß√£o do Ollama:
‚úÖ Ollama instalado: $(ollama --version 2>/dev/null || echo "N/A")
‚úÖ Servi√ßo rodando: $(pgrep -f "ollama serve" > /dev/null && echo "SIM" || echo "N√ÉO")
‚úÖ URL: ${OLLAMA_API_URL:-"http://localhost:11434/api/chat"}
‚úÖ Modelo: ${OLLAMA_MODEL:-"$RECOMMENDED_MODEL"}

Modelos dispon√≠veis:
$MODELS

Teste de conectividade:
‚úÖ API Version: $(curl -s "$BASE_URL/api/version" 2>/dev/null || echo "ERRO")
‚úÖ Modelo testado: $TEST_MODEL
‚úÖ Resposta teste: $TEST_RESPONSE

Configura√ß√£o Django:
‚úÖ Vari√°veis ambiente: .env configurado
‚úÖ URLs: api/ inclu√≠do
‚úÖ View: llama_assistant dispon√≠vel

Pr√≥ximos passos:
1. Reiniciar Django: sudo systemctl restart labconnect
2. Testar chatbot: /api/assistant/
3. Verificar logs: tail -f logs/labconnect.log

Comandos √∫teis:
- Ver modelos: ollama list
- Baixar modelo: ollama pull llama3
- Testar Ollama: curl http://localhost:11434/api/version
- Logs Ollama: tail -f /tmp/ollama.log

Troubleshooting:
- Se Ollama n√£o responder: ollama serve
- Se modelo n√£o funcionar: ollama pull $RECOMMENDED_MODEL
- Se Django der erro: verificar logs e URLs
EOF

log "‚úÖ Relat√≥rio salvo em: chatbot-config-report.txt"

echo ""
echo -e "${BLUE}üéâ CONFIGURA√á√ÉO DO CHATBOT CONCLU√çDA! üéâ${NC}"
echo ""
echo -e "${GREEN}Resumo da configura√ß√£o:${NC}"
echo "‚Ä¢ Ollama instalado e rodando"
echo "‚Ä¢ Modelo $RECOMMENDED_MODEL configurado"
echo "‚Ä¢ Vari√°veis de ambiente definidas"
echo "‚Ä¢ Integra√ß√£o Django testada"
echo ""
echo -e "${YELLOW}Para testar o chatbot:${NC}"
echo "1. Reinicie o Django: sudo systemctl restart labconnect"
echo "2. Acesse: https://seu-dominio/api/assistant/"
echo "3. Digite uma mensagem de teste"
echo ""
echo -e "${BLUE}URLs do chatbot:${NC}"
echo "‚Ä¢ API Endpoint: /api/assistant/"
echo "‚Ä¢ P√°gina de teste: /api/test-chatbot/"
echo ""
echo -e "${GREEN}O chatbot est√° pronto para uso! üöÄ${NC}"